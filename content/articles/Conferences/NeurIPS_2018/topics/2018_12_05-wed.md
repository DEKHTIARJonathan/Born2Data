Title: NeurIPS 2018 - Research Log
HeadTitle: NeurIPS 2018 - Research Log
Date: 2018-12-03 10:15
Category: Deep Learning
Tags: Deep Learning, Research, Statistics, Computer Vision, NLP, NIPS, NeurIPS
Slug: NeurIPS_2018/topics/2018_12_05-wed
Author: Jonathan DEKHTIAR
Headline: A condensed research review of the talks and poster sessions I attended.
Status: draft

# Wednesday 05 December 2018
--------------------

### 1. Variance-Reduced Stochastic Gradient Descent on Streaming Data

**Link to paper:** <https://papers.nips.cc/paper/8196-variance-reduced-stochastic-gradient-descent-on-streaming-data>

----------------------

### 2. Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units

**Link to paper:** <https://arxiv.org/abs/1810.01877>

----------------------

### 3. Learning Sparse Neural Networks via Sensitivity-Driven Regularization

**Link to paper:** <https://arxiv.org/abs/1810.11764>

----------------------

### 4. DropBlock: A regularization method for convolutional networks

**Link to paper:** <https://arxiv.org/abs/1810.12890>

**Unofficial Implementation 1 (TF):** <https://github.com/DHZS/tf-dropblock>

**Unofficial Implementation 2 (PyT):** <https://github.com/miguelvr/dropblock>

**Unofficial Implementation 3 (PyT):** <https://github.com/Randl/DropBlock-pytorch>

**Unofficial Implementation 4 (PyT):** <https://github.com/darkknightzh/DropBlock_pytorch>

----------------------

### 5. Simple, Distributed, and Accelerated Probabilistic Programming - Google AI

**Link to paper:** <https://arxiv.org/abs/1811.02091>

----------------------

### 6. Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming - Google AI

**Link to paper:** <https://arxiv.org/abs/1809.09569>

**Github Repo:** <https://github.com/google/tangent>

----------------------

### 7. On the Local Hessian in Back-propagation - Microsoft

**Link to paper:** <https://papers.nips.cc/paper/7887-on-the-local-hessian-in-back-propagation>

----------------------

### 8. Neural Arithmetic Logic Units - DeepMind

**Link to paper:** <https://arxiv.org/abs/1808.00508>

**Github Repo:** <https://github.com/ahylton19/simpleNALU-tf>

**TF Blog Post:** <https://medium.com/tensorflow/understanding-neural-arithmetic-logic-units-11b0f85c1d1d>

Numerous third party blog posts are available on that one.

**Blog Post 1:** <https://medium.com/mlreview/simple-guide-to-neural-arithmetic-logic-units-nalu-explanation-intuition-and-code-64bc22605712>

**Blog Post 2:** <https://medium.com/analytics-vidhya/neural-arithmetic-logic-units-nalu-a-new-beginning-9b9b8a69eb32>

**Blog Post 3:** <https://sergioskar.github.io/NALU/>

**Youtube Video by Siraj:** <https://www.youtube.com/watch?v=v9E7Wg0dHiU>

----------------------

### 9. Porcupine Neural Networks: Approximating Neural Network Landscapes

**Link to paper:** <https://papers.nips.cc/paper/7732-porcupine-neural-networks-approximating-neural-network-landscapes>

----------------------

### 10. Adding One Neuron Can Eliminate All Bad Local Minima

**Link to paper:** <https://arxiv.org/abs/1805.08671>

**Unofficial Summary:** <https://www.techleer.com/articles/526-adding-one-neuron-can-eliminate-all-bad-local-minima/>

**Unofficial blog post:** <https://rossum.ai/blog/2018/07/22/does-adding-one-neuron-help-real-world-networks/>

----------------------

### 10. Sequential Attend Infer Repeat: Generative Modelling of Moving Objects

**Link to paper:** <>

----------------------

### 11. Sanity Checks for Saliency Maps

**Link to paper:** <>

----------------------

### 12. Sparsified SGD with memory

**Link to paper:** <>

----------------------

### 13. ATOMO: Communication-efficient learning via atomic Sparsification

**Link to paper:** <>

----------------------

### 14. Dimensionality Reduction has quantifiable imperfections: two geometric bounds

**Link to paper:** <>

----------------------

### 15. Deep non blind deconvolution via generalized low rank approximation

**Link to paper:** <>

----------------------

### 16. A loss framework for calibrated anomaly detection

**Link to paper:** <>

----------------------

### 17. How to start training: the effect of initialization and architecture

**Link to paper:** <>

----------------------

### 18. A Probabilistic U-Net Segmentation of ambiguous images

**Link to paper:** <>

----------------------

### 19. Learning Pipelines with limited data and domain knowledge: a study in parsing physics problems

**Link to paper:** <>

----------------------

### 20. Pipe SGD: A decentralized pipelined SGD framework for distributed deep net training

**Link to paper:** <>

----------------------

### 21. Sublinear time low rank approximation of distance matrices

**Link to paper:** <>

----------------------

### 22. Legendre Decompisition for Tensors

**Link to paper:** <>

----------------------

### 23. Scalable Laplacian K-modes

**Link to paper:** <>

----------------------

### 24. Reducing Netzork Agnostophobia

**Link to paper:** <>

----------------------

### 25. Physical Systems behind optimization algorithms

**Link to paper:** <>

----------------------

### 26. Benefits of over parametrization with EM

**Link to paper:** <>

----------------------

### 27. Model based targeted Dimensionality reduction for neuronal population data

**Link to paper:** <>
Open Source !

----------------------

### 28. Structured Local minima in Sparse blind deconvolution

**Link to paper:** <>

----------------------

### 29. Are ResNets provably better than linear predictors ?

**Link to paper:** <>

----------------------

### 30. GILBO: One Metric to rule them all

**Link to paper:** <>

----------------------

### 31. Distributed Stochastic Optimization via adaptive SGD

**Link to paper:** <>

----------------------

### 32. The effect of network width on the performance of large batch training

**Link to paper:** <>

----------------------

### 33. GPytorch: Blackbox matrix-matrix gaussian process inference with GPU acceleration

**Link to paper:** <>



=====================================================================

----------------------

### 666.

**Link to paper:** <>

**Blog Post:** <>

**Github Repo:** <>
