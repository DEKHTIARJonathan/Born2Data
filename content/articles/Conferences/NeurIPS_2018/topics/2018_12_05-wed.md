Title: NeurIPS 2018 - Research Log
HeadTitle: NeurIPS 2018 - Research Log
Date: 2018-12-03 10:15
Category: Deep Learning
Tags: Deep Learning, Research, Statistics, Computer Vision, NLP, NIPS, NeurIPS
Slug: NeurIPS_2018/topics/2018_12_05-wed
Author: Jonathan DEKHTIAR
Headline: A condensed research review of the talks and poster sessions I attended.
Status: draft

# Wednesday 05 December 2018
--------------------

### 1. Variance-Reduced Stochastic Gradient Descent on Streaming Data

**Link to paper:** <https://papers.nips.cc/paper/8196-variance-reduced-stochastic-gradient-descent-on-streaming-data>

----------------------

### 2. Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units

**Link to paper:** <https://arxiv.org/abs/1810.01877>

----------------------

### 3. Learning Sparse Neural Networks via Sensitivity-Driven Regularization

**Link to paper:** <https://arxiv.org/abs/1810.11764>

----------------------

### 4. DropBlock: A regularization method for convolutional networks

**Link to paper:** <https://arxiv.org/abs/1810.12890>

**Unofficial Implementation 1 (TF):** <https://github.com/DHZS/tf-dropblock>

**Unofficial Implementation 2 (PyT):** <https://github.com/miguelvr/dropblock>

**Unofficial Implementation 3 (PyT):** <https://github.com/Randl/DropBlock-pytorch>

**Unofficial Implementation 4 (PyT):** <https://github.com/darkknightzh/DropBlock_pytorch>

----------------------

### 5. Simple, Distributed, and Accelerated Probabilistic Programming - Google AI

**Link to paper:** <https://arxiv.org/abs/1811.02091>

----------------------

### 6. Tangent: Automatic differentiation using source-code transformation for dynamically typed array programming - Google AI

**Link to paper:** <https://arxiv.org/abs/1809.09569>

**Github Repo:** <https://github.com/google/tangent>

----------------------

### 7. On the Local Hessian in Back-propagation - Microsoft

**Link to paper:** <https://papers.nips.cc/paper/7887-on-the-local-hessian-in-back-propagation>

----------------------

### 8. Neural Arithmetic Logic Units - DeepMind

**Link to paper:** <https://arxiv.org/abs/1808.00508>

**Github Repo:** <https://github.com/ahylton19/simpleNALU-tf>

**TF Blog Post:** <https://medium.com/tensorflow/understanding-neural-arithmetic-logic-units-11b0f85c1d1d>

Numerous third party blog posts are available on that one.

**Blog Post 1:** <https://medium.com/mlreview/simple-guide-to-neural-arithmetic-logic-units-nalu-explanation-intuition-and-code-64bc22605712>

**Blog Post 2:** <https://medium.com/analytics-vidhya/neural-arithmetic-logic-units-nalu-a-new-beginning-9b9b8a69eb32>

**Blog Post 3:** <https://sergioskar.github.io/NALU/>

**Youtube Video by Siraj:** <https://www.youtube.com/watch?v=v9E7Wg0dHiU>

----------------------

### 9. Porcupine Neural Networks: Approximating Neural Network Landscapes

**Link to paper:** <https://papers.nips.cc/paper/7732-porcupine-neural-networks-approximating-neural-network-landscapes>

----------------------

### 10. Adding One Neuron Can Eliminate All Bad Local Minima

**Link to paper:** <https://arxiv.org/abs/1805.08671>

**Unofficial Summary:** <https://www.techleer.com/articles/526-adding-one-neuron-can-eliminate-all-bad-local-minima/>

**Unofficial blog post:** <https://rossum.ai/blog/2018/07/22/does-adding-one-neuron-help-real-world-networks/>

----------------------

### 10.

**Link to paper:** <>

----------------------

### 11.

**Link to paper:** <>

----------------------

### 12.

**Link to paper:** <>

----------------------

### 13.

**Link to paper:** <>

----------------------

### 14.

**Link to paper:** <>

----------------------

### 15.

**Link to paper:** <>

----------------------

### 16.

**Link to paper:** <>

----------------------

### 17.

**Link to paper:** <>

----------------------

### 18.

**Link to paper:** <>

----------------------

### 19.

**Link to paper:** <>

----------------------

### 20.

**Link to paper:** <>

----------------------

### 21.

**Link to paper:** <>

----------------------

### 22.

**Link to paper:** <>



=====================================================================

----------------------

### 666. Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?

**Link to paper:** <>

**Blog Post:** <>

**Github Repo:** <>
