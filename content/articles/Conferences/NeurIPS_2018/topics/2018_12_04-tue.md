Title: NeurIPS 2018 - Research Log
HeadTitle: NeurIPS 2018 - Research Log
Date: 2018-12-03 10:15
Category: Deep Learning
Tags: Deep Learning, Research, Statistics, Computer Vision, NLP, NIPS, NeurIPS
Slug: NeurIPS_2018/topics/2018_12_04-tue
Author: Jonathan DEKHTIAR
Headline: A condensed research review of the talks and poster sessions I attended.
Status: draft

# Tuesday 04 December 2018
--------------------

### 1. Faster Neural Networks Straight from JPEG - Uber AI Labs

- **Link to paper:** <https://is.gd/CNN_from_JPEG>
<!-- https://is.gd/stats.php?url=CNN_from_JPEG -->

- **Useful code to reproduce:** <https://github.com/uber-research/jpeg2dct>


**Core idea:** instead of feeding pixel values in the CNN, they fed the DCT coefficients from the JPEG-encoded version of the image.

**Benefits:**

- **Computation Related:** Save some data loading computation as we don't have to decode the image.

- **Memory related:** The JPEG Encoded image is already a compressed representation of the image.
Thus saving memory and which could also facilitate learning.

__Claims a 77% speed increase on ResNet 50 while keeping the same accuracy.__
It shall be crosschecked with efficient data loading pipeline such as DALI, probably we can expect a bit less, still it looks promising).

**Limitation:**

* **Data Augmentation:** as it is impossible (to the best of my knowledge) to perform any data augmentation (crop, brightness, contrast, etc...). This enforce a quite inefficient pipeline at training:
    1. JPEG_Decode
    2. Augment (possibly N times)
    3. JPEG_Encode => Access to DCT coefficients.

----------------------

### 2. An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution - Uber AI Labs

**Link to paper:** <https://arxiv.org/abs/1807.03247/>

**Blog Post:** <https://eng.uber.com/coordconv/>

----------------------

### 3. Which Neural Net Architectures Give Rise To Exploding and Vanishing Gradients?

**Link to paper:** <https://arxiv.org/abs/1801.03744/>


----------------------

### 4. Scalable Methods for 8-bit Training of Neural Networks

**Link to paper:** <https://arxiv.org/abs/1805.11046>

**Blog Post:** <https://ai.intel.com/scalable-methods-for-8-bit-training-of-neural-networks-blog/>

**Github Repo:** <https://github.com/eladhoffer/quantized.pytorch>


----------------------

### 5. A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication

**Link to paper:** <https://is.gd/quant_sparse_comm>
<!-- https://is.gd/stats.php?url=quant_sparse_comm -->


----------------------

### 6. Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks

**Link to paper:** <https://arxiv.org/abs/1805.07925>

**Github Repo:** <https://github.com/hyeonseob-nam/Batch-Instance-Normalization>


----------------------

### 7. BML: A High-performance, Low-cost Gradient Synchronization Algorithm for DML Training

**Link to paper:** <https://is.gd/dml_training>
<!-- https://is.gd/stats.php?url=dml_training -->

*Need to test*


----------------------

### 8. ChannelNets: Compact and Efficient Convolutional Neural Networks via Channel-Wise Convolutions

**Link to paper:** <https://arxiv.org/abs/1809.01330>

**Github Repo:** <https://github.com/HongyangGao/ChannelNets>


----------------------

### 9. Heterogeneous Bitwidth Binarization in Convolutional Neural Networks

**Link to paper:** <https://arxiv.org/abs/1805.10368>

**Github Repo:** *Coming soon in PyTorch*


----------------------

### 10. Bayesian Distributed Stochastic Gradient Descent

**Link to paper:** <https://is.gd/bayes_dist_sgd>
<!-- https://is.gd/stats.php?url=bayes_dist_sgd -->

----------------------

### 11. Frequency-Domain Dynamic Pruning for Convolutional Neural Networks

**Link to paper:** <https://is.gd/freq_prune_CNN>
<!-- https://is.gd/stats.php?url=freq_prune_CNN -->

----------------------

### 12. Kalman Normalization: Normalizing Internal Representations Across Network Layers

**Link to paper:** <https://is.gd/kalman_norm>
<!-- https://is.gd/stats.php?url=kalman_norm -->

**Related study:** <https://arxiv.org/abs/1802.03133>

----------------------

### 13. Moonshine: Distilling with Cheap Convolutions

**Link to paper:** <https://arxiv.org/abs/1711.02613>

**Github Repo:** <https://github.com/BayesWatch/pytorch-moonshine>

----------------------

### 14. Deep Neural Networks with Box Convolutions

**Link to paper:** <https://is.gd/box_conv>
<!-- https://is.gd/stats.php?url=box_conv -->

**Paper Summary (by 1st author):** <https://gist.github.com/shrubb/f131e99eb3352cdebe506d2ad988c1b7>

----------------------

### 16. Mesh-TensorFlow: Deep Learning for Supercomputers

**Link to paper:** <https://arxiv.org/abs/1811.02084>

**Github Repo:** <https://github.com/tensorflow/mesh/tree/master/mesh_tensorflow>

----------------------

### 17. Visualizing the Loss Landscape of Neural Nets

**Link to paper:** <https://arxiv.org/abs/1712.09913>

**Github Repo:** <https://github.com/tomgoldstein/loss-landscape>

----------------------

### 18. Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data

**Link to paper:** <https://arxiv.org/abs/1808.01204>

**Slides:** <https://is.gd/overpara_SGD>
<!-- https://is.gd/stats.php?url=overpara_SGD -->

----------------------

### 19. Gradient Sparsification for Communication-Efficient Distributed Optimization

**Link to paper:** <https://arxiv.org/abs/1710.09854>

----------------------

### 20. GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training

**Link to paper:** <https://arxiv.org/abs/1811.03617>
