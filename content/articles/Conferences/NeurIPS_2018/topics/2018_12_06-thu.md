Title: NeurIPS 2018 - Research Log
HeadTitle: NeurIPS 2018 - Research Log
Date: 2018-12-03 10:15
Category: Deep Learning
Tags: Deep Learning, Research, Statistics, Computer Vision, NLP, NIPS, NeurIPS
Slug: NeurIPS_2018/topics/2018_12_06-thu
Author: Jonathan DEKHTIAR
Headline: A condensed research review of the talks and poster sessions I attended.
Status: draft

# Wednesday 05 December 2018
--------------------

### 1. How Many Samples are Needed to Estimate a Convolutional Neural Network?

**Link to paper:** <https://arxiv.org/abs/1805.07883>

----------------------

### 2. Automatic differentiation in ML: Where we are and where we should be going

**Link to paper:** <https://arxiv.org/abs/1810.11530>

----------------------

### 3. Realistic Evaluation of Deep Semi-Supervised Learning Algorithms

**Link to paper:** <https://arxiv.org/abs/1804.09170>

**Github Repository:** <https://github.com/brain-research/realistic-ssl-evaluation>

----------------------

### 4. Generalisation in humans and deep neural networks

**Link to paper:** <https://arxiv.org/abs/1808.08750>

**Github Repository:** <https://github.com/rgeirhos/generalisation-humans-DNNs>

----------------------

### 5. Implicit Bias of Gradient Descent on Linear Convolutional Networks

**Link to paper:** <https://arxiv.org/abs/1806.00468>

----------------------

### 6. Visual Object Networks: Image Generation with Disentangled 3D Representation

**Link to paper:** <https://arxiv.org/abs/1812.02725>

**Github Repository:** <https://github.com/junyanz/VON>

**Blog Post:** <http://von.csail.mit.edu/>

----------------------

### 7. Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?

**Link to paper:** <https://arxiv.org/abs/1810.09102>

**Github Repository:** <https://github.com/nbansal90/Can-we-Gain-More-from-Orthogonality>

----------------------

### 8. Discrimination-aware Channel Pruning for Deep Neural Networks

**Link to paper:** <https://arxiv.org/abs/1810.11809>

**Github Repository:** <https://github.com/SCUT-AILab/Discrimination-aware-Channel-Pruning-for-Deep-Neural-Networks>

----------------------

### 9. Communication Compression for Decentralized Training

**Link to paper:** <https://arxiv.org/abs/1803.06443>

----------------------

### 10. FastGRNN: A Fast, Accurate, Stable and Tiny Kilobyte Sized Gated Recurrent Neural Network

**Link to paper:** <https://is.gd/FastGRNN>
<!-- https://is.gd/stats.php?url=FastGRNN -->

**Link to poster:** <https://adityakusupati.github.io/docs/FastGRNNPoster.pdf>

**Github Repository:** <https://github.com/Microsoft/EdgeML/>

**Youtube Video (3min):** <https://www.youtube.com/watch?v=3ZpCnOWBrio>

----------------------

### 11. Understanding Batch Normalization

**Link to paper:** <https://arxiv.org/abs/1806.02375>

----------------------

### 12. Automating Bayesian optimization with Bayesian optimization

**Link to paper:** <https://is.gd/bayes_opt_with_bayes_opt>
<!-- https://is.gd/stats.php?url=bayes_opt_with_bayes_opt -->

----------------------

### 13. BRUNO: A Deep Recurrent Model for Exchangeable Data

**Link to paper:** <https://arxiv.org/abs/1802.07535>

**Github Repository:** <https://github.com/IraKorshunova/bruno>

**Blog Post:** <https://irakorshunova.github.io/blog/bruno>

----------------------

### 14. Reparameterization Gradient for Non-differentiable Models

**Link to paper:** <https://arxiv.org/abs/1806.00176>

----------------------

### 15. Flexible and accurate inference and learning for deep generative models

**Link to paper:** <https://arxiv.org/abs/1805.11051>

----------------------

### 16. Multi-value Rule Sets for Interpretable Classification with Feature-Efficient Representations

**Link to paper:** <https://is.gd/feature_efficient_repr>
<!-- https://is.gd/stats.php?url=feature_efficient_repr -->

----------------------

### 17. Why Is My Classifier Discriminatory?

**Link to paper:** <https://arxiv.org/abs/1805.12002>

----------------------

### 18. Learning to Teach with Dynamic Loss Functions

**Link to paper:** <https://arxiv.org/abs/1810.12081>

----------------------

### 19. Transfer Learning with Neural AutoML

**Link to paper:** <https://arxiv.org/abs/1803.02780>

----------------------

### 20. Binary Classification from Positive-Confidence Data

**Link to paper:** <https://arxiv.org/abs/1710.07138>

----------------------

### 21. Learning SMaLL Predictors

**Link to paper:** <https://arxiv.org/abs/1803.02388>

----------------------

### 22. Informative Features for Model Comparison

**Link to paper:** <https://is.gd/features_for_model_comp>
<!-- https://is.gd/stats.php?url=features_for_model_comp -->

**Link to Poster:** <http://wittawat.com/assets/poster/kmod_nips2018_poster.pdf>

----------------------

### 23. Low-rank Interaction with Sparse Additive Effects Model for Large Data Frames

**Link to paper:** <https://is.gd/sparse_additive_effect>
<!-- https://is.gd/stats.php?url=sparse_additive_effect -->

----------------------

### 24. Fully Understanding the Hashing Trick

**Link to paper:** <https://arxiv.org/abs/1805.08539>

**Youtube Video (30min)**: <https://www.youtube.com/watch?v=7jseJ-shYWc>

----------------------

### 25. Efficient Anomaly Detection via Matrix Sketching

**Link to paper:** <https://arxiv.org/abs/1804.03065>

**Youtube Video (3min)**: <https://www.youtube.com/watch?v=qcOP15CH3DA>

----------------------

### 26. On Controllable Sparse Alternatives to Softmax

**Link to paper:** <https://arxiv.org/abs/1810.11975>

----------------------

### 27. Exploiting Numerical Sparsity for Efficient Learning : Faster Eigenvector Computation and Regression

**Link to paper:** <https://arxiv.org/abs/1811.10866>

----------------------

### 28. Boosted Sparse and Low-Rank Tensor Regression

**Link to paper:** <https://arxiv.org/abs/1811.01158>

**Github Repository:** <https://github.com/LifangHe/SURF>

----------------------

### 29. The Convergence of Sparsified Gradient Methods

**Link to paper:** <https://arxiv.org/abs/1809.10505>

----------------------

### 30. Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise

**Link to paper:** <https://arxiv.org/abs/1802.05300>

**Github Repository:** <https://github.com/mmazeika/glc>

----------------------

### 31. Norm matters: efficient and accurate normalization schemes in deep networks

**Link to paper:** <https://arxiv.org/abs/1803.01814>

----------------------

### 32. Constructing Fast Network through Deconstruction of Convolution

**Link to paper:** <https://arxiv.org/abs/1806.07370>

----------------------

### 33. LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning
**Link to paper:** <https://arxiv.org/abs/1805.09965>

----------------------

### 34. Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels

**Link to paper:** <https://arxiv.org/abs/1805.07836>

----------------------

### 35. CatBoost: unbiased boosting with categorical features

**Link to paper:** <https://arxiv.org/abs/1706.09516>

----------------------


=====================================================================

----------------------

### 666.

**Link to paper:** <>

**Blog Post:** <>

**Github Repo:** <>
