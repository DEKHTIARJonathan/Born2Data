Title: NeurIPS 2018 - Research Log
HeadTitle: NeurIPS 2018 - Research Log
Date: 2018-12-03 10:15
Category: Deep Learning
Tags: Deep Learning, Research, Statistics, Computer Vision, NLP, NIPS, NeurIPS
Slug: NeurIPS_2018/topics/2018_12_06-thu
Author: Jonathan DEKHTIAR
Headline: A condensed research review of the talks and poster sessions I attended.
Status: draft

# Wednesday 05 December 2018
--------------------

### 1. How many samples are needed to estimate a convolutional neural network ?

**Link to paper:** <>

----------------------

### 2. Automatic differentiation in machine learning: where we are and where we should be going ?

**Link to paper:** <>

----------------------

### 3. Realistic Evaluation of Deep Semi Supervised Learning Algorithms

**Link to paper:** <>

----------------------

### 4. Generalisation in humans and deep neural networks

**Link to paper:** <>

----------------------

### 5. Implicit Bias of Gradient Descent on Linear Convolutional Networks

**Link to paper:** <>

----------------------

### 6. Visual Object Networks: Image Generation with Disentangled 3D Representations

**Link to paper:** <>

----------------------

### 7. Can we gain more from Orthogonality Regularizations in Training deep convolutional networks

**Link to paper:** <>

----------------------

### 8. Discrimination aware channel pruning for deep neural networks.

**Link to paper:** <>

----------------------

### 9. Communication Compression for decentralized training

**Link to paper:** <>

----------------------

### 10. FastGRNN: A fast, accurate, stable and tiny kilobyte sized recurrent neural network

**Link to paper:** <>
Open source !

----------------------

### 11. Understqnding Batch Normalization

**Link to paper:** <>

----------------------

### 12. Automating Bayesian Optimization with Bayesian Optimization

**Link to paper:** <>

----------------------

### 13. BRUNO: A deep recurrent model for exchangeable data

**Link to paper:** <>

----------------------

### 14. Reparametrization gradient for non differentiable models

**Link to paper:** <>

----------------------

### 15. Flexible and accurate inference and learning for deep Generative models

**Link to paper:** <>

----------------------

### 16. Multi value rule sets for interpretable classification with feature efficient representations

**Link to paper:** <>

----------------------

### 17. Multivariate convolutional sparse coding for electromagnetic brain signals

**Link to paper:** <>

----------------------

### 18. Why is my classifier Discriminatory ?

**Link to paper:** <>

----------------------

### 19. Learning to teach with dynamic loss functions

**Link to paper:** <>

----------------------

### 20. Transfer learning with neural AutoML

**Link to paper:** <>

----------------------

### 21. Binary Classifier from positive-confidence data

**Link to paper:** <>

----------------------

### 22. Learning SMaLL Predictors

**Link to paper:** <>

----------------------

### 23. Informative Features for Model Compression

**Link to paper:** <>

----------------------

### 24. Low Rank Interaction with sparse additive effects model for large data frames

**Link to paper:** <>

----------------------

### 25. Fully Understanding the hashing trick

**Link to paper:** <>

----------------------

### 26. Efficient Anomaly Detection using matrix sketching

**Link to paper:** <>

----------------------

### 27. On Controllable Sparse Alternatives to Softmax

**Link to paper:** <>

----------------------

### 28. Exploiting Numerical Sparsity for Efficient Learning: Faster Eigenvector computation and regression

**Link to paper:** <>

----------------------

### 29. Boosted Sparse and Low Rank Tensor Regression

**Link to paper:** <>

----------------------

### 30. The convergence of sparsified gradient methods

**Link to paper:** <>

----------------------

### 31. Using Trusted data to train deep networks on labels corrupted by severe noise

**Link to paper:** <>

----------------------

### 32. Norm Matters: efficient and accurate normalization schenes in deep networks

**Link to paper:** <>

----------------------

### 33. Constructing Fast Networks through deconstruction of convolution

**Link to paper:** <>

----------------------

### 34. LAG: Lazily Aggregated Gradient for Communication-Efficient Distributed Learning
**Link to paper:** <>

----------------------

### 35. Generalized Cross Entropy for Training Deep Neural Networks with Noisy Labels

**Link to paper:** <>

----------------------

### 36. CatBoost: unbiased boosting with categorical features

**Link to paper:** <>

----------------------


=====================================================================

----------------------

### 666.

**Link to paper:** <>

**Blog Post:** <>

**Github Repo:** <>
